# FlashAttention Lite (OpenAI Triton)

A learning-focused project to understand modern GPU kernel development using Triton compiler.

## ğŸ¯ Learning Goals

By completing this project, you will understand:
- Triton compiler and Python-like GPU programming
- FlashAttention algorithm (tiling Q, K, V in SRAM)
- Block-level memory management
- Online softmax computation
- Memory-efficient attention for long sequences

## âš ï¸ Prerequisites

- **Triton**: `pip install triton`
- **NVIDIA GPU**: Compute Capability 7.0+ (Volta or newer)
- **Python 3.8+**

## ğŸ“š Learning Roadmap

### Phase 1: Theory (Notes)

| # | Note | Topic | Status |
|---|------|-------|--------|
| 0 | [00_triton_basics.md](notes/00_triton_basics.md) | Triton vs CUDA, blocks, programming model | â¬œ |
| 1 | [01_flash_attention_algorithm.md](notes/01_flash_attention_algorithm.md) | FlashAttention tiling and online softmax | â¬œ |
| 2 | [02_memory_efficiency.md](notes/02_memory_efficiency.md) | HBM reduction, IO complexity | â¬œ |

### Phase 2: Implementation

| # | File | Concept | Prereq Notes | Status |
|---|------|---------|--------------|--------|
| 1 | [01_pytorch_attention.py](src/01_pytorch_attention.py) | Baseline PyTorch attention | - | â¬œ |
| 2 | [02_triton_matmul.py](src/02_triton_matmul.py) | Simple Triton matmul (warmup) | 0 | â¬œ |
| 3 | [03_triton_softmax.py](src/03_triton_softmax.py) | Triton fused softmax | 0 | â¬œ |
| 4 | [04_flash_attention.py](src/04_flash_attention.py) | Full FlashAttention kernel | 0, 1 | â¬œ |

### Phase 3: Benchmarking

| # | File | Purpose | Status |
|---|------|---------|--------|
| 1 | [benchmark.py](benchmarks/benchmark.py) | Compare latency and memory | â¬œ |

## ğŸ“ Directory Structure

```
triton-flash-attention-lite/
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ 01_pytorch_attention.py   # Baseline
â”‚   â”œâ”€â”€ 02_triton_matmul.py       # Warmup: simple matmul
â”‚   â”œâ”€â”€ 03_triton_softmax.py      # Fused softmax
â”‚   â””â”€â”€ 04_flash_attention.py     # Full FlashAttention
â”œâ”€â”€ notes/
â”‚   â”œâ”€â”€ 00_triton_basics.md
â”‚   â”œâ”€â”€ 01_flash_attention_algorithm.md
â”‚   â””â”€â”€ 02_memory_efficiency.md
â””â”€â”€ benchmarks/
    â””â”€â”€ benchmark.py
```

## ğŸ”§ Setup & Run

```bash
# Install Triton
pip install triton

# Run baseline
python src/01_pytorch_attention.py

# Run FlashAttention
python src/04_flash_attention.py

# Benchmark
python benchmarks/benchmark.py --seq_len 4096
```

## ğŸ“Š Expected Results

For sequence length N=4096, head_dim=64:

| Implementation | Time | Memory | Speedup |
|----------------|------|--------|---------|
| PyTorch Standard | ~50ms | O(NÂ²) | 1x |
| Triton FlashAttention | ~10ms | O(N) | 5x |

## ğŸ”— Relevance

**Industry Skills**: Triton is used by Meta, OpenAI for production kernels. Understanding compiler-based GPU programming is a "unicorn skill."

## ğŸ“– References

- [Triton Documentation](https://triton-lang.org/)
- [FlashAttention Paper](https://arxiv.org/abs/2205.14135)
- [FlashAttention-2](https://arxiv.org/abs/2307.08691)
- [OpenAI Triton Tutorials](https://triton-lang.org/main/getting-started/tutorials/)
